{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-19T19:54:06.196630Z",
     "start_time": "2026-01-19T19:54:05.428220Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$",
   "id": "9c81e0f0533c9d51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T19:54:51.983184Z",
     "start_time": "2026-01-19T19:54:51.957651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    '''\n",
    "    Scaled Dot Product Attention\n",
    "    attention(Q,K,V) = softmax((Q*K^T)/sqrt(d_k))*V\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        '''\n",
    "\n",
    "        :param query: 查询向量，[batch_size, num_heads, seq_len, d_model]\n",
    "        :param key:  键值向量， [batch_size, num_heads, seq_len, d_model]\n",
    "        :param value: 值向量， [batch_size, num_heads, seq_len, d_model]\n",
    "        :param mask [[1,0,0],[1,1,0], [1,1,1]]\n",
    "        :return:\n",
    "        '''\n",
    "        d_k= key.size()[-1]\n",
    "        # scores [batch_size, seq_len, seq_len]\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "        #mask:give a A negative epsilon\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0, -1e9)\n",
    "        scores = torch.softmax(scores, dim=-1)\n",
    "        # output [batch_size, seq_len, d_model]  broadcast\n",
    "        output = torch.matmul(scores, value)\n",
    "\n",
    "        return output, scores"
   ],
   "id": "351dad32d59aaed0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T19:54:55.052526Z",
     "start_time": "2026-01-19T19:54:54.967819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sdpa = ScaledDotProductAttention()\n",
    "\n",
    "batch_size, seq_len, d_model = 16, 10, 768\n",
    "\n",
    "query = torch.randn(batch_size, seq_len, d_model)\n",
    "key = torch.randn(batch_size, seq_len, d_model)\n",
    "value = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, scores = sdpa(query, key, value)\n",
    "\n",
    "print(f\"output size is {output.size()}\")\n",
    "print(f\"scores size is {scores.size()}\")\n",
    "print(f\"score size is {scores}\")"
   ],
   "id": "b381bb8ef7df8a63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size is torch.Size([16, 10, 768])\n",
      "scores size is torch.Size([16, 10, 10])\n",
      "score size is tensor([[[0.0650, 0.0439, 0.1345,  ..., 0.0685, 0.0072, 0.0500],\n",
      "         [0.1130, 0.0637, 0.0620,  ..., 0.0549, 0.1023, 0.1027],\n",
      "         [0.0322, 0.0253, 0.3055,  ..., 0.2461, 0.0118, 0.1107],\n",
      "         ...,\n",
      "         [0.1474, 0.0704, 0.0691,  ..., 0.0969, 0.0719, 0.0782],\n",
      "         [0.6037, 0.0083, 0.0951,  ..., 0.0349, 0.0395, 0.0488],\n",
      "         [0.0390, 0.0444, 0.5200,  ..., 0.0556, 0.1627, 0.0166]],\n",
      "\n",
      "        [[0.1081, 0.0291, 0.2011,  ..., 0.0379, 0.1862, 0.0156],\n",
      "         [0.1251, 0.0318, 0.0303,  ..., 0.0689, 0.0555, 0.1269],\n",
      "         [0.1809, 0.0568, 0.0204,  ..., 0.0760, 0.0344, 0.0459],\n",
      "         ...,\n",
      "         [0.0044, 0.0211, 0.0781,  ..., 0.1824, 0.0252, 0.0061],\n",
      "         [0.0128, 0.0626, 0.1920,  ..., 0.0288, 0.1591, 0.0170],\n",
      "         [0.0903, 0.0451, 0.2762,  ..., 0.0544, 0.0705, 0.0963]],\n",
      "\n",
      "        [[0.0161, 0.0658, 0.0727,  ..., 0.0330, 0.3254, 0.2031],\n",
      "         [0.0089, 0.0446, 0.1783,  ..., 0.0860, 0.1142, 0.3642],\n",
      "         [0.0290, 0.2324, 0.1761,  ..., 0.0416, 0.2074, 0.0138],\n",
      "         ...,\n",
      "         [0.0423, 0.0716, 0.0712,  ..., 0.0718, 0.0168, 0.4032],\n",
      "         [0.0380, 0.1326, 0.0253,  ..., 0.0876, 0.1153, 0.0985],\n",
      "         [0.1140, 0.0803, 0.1416,  ..., 0.0423, 0.1479, 0.1398]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0770, 0.0182, 0.0878,  ..., 0.1056, 0.0250, 0.1802],\n",
      "         [0.1312, 0.0333, 0.1121,  ..., 0.4035, 0.0372, 0.1208],\n",
      "         [0.0384, 0.6678, 0.0163,  ..., 0.0577, 0.0348, 0.0563],\n",
      "         ...,\n",
      "         [0.1385, 0.1230, 0.1561,  ..., 0.0479, 0.0332, 0.2049],\n",
      "         [0.0386, 0.0501, 0.0746,  ..., 0.1328, 0.2357, 0.0695],\n",
      "         [0.0354, 0.3169, 0.0580,  ..., 0.0722, 0.0229, 0.0704]],\n",
      "\n",
      "        [[0.0743, 0.0360, 0.3236,  ..., 0.0166, 0.0378, 0.0117],\n",
      "         [0.0750, 0.0704, 0.2817,  ..., 0.2445, 0.0258, 0.1162],\n",
      "         [0.0426, 0.0116, 0.0528,  ..., 0.0303, 0.0213, 0.0926],\n",
      "         ...,\n",
      "         [0.0599, 0.0028, 0.1442,  ..., 0.1355, 0.0939, 0.0446],\n",
      "         [0.2282, 0.0482, 0.0259,  ..., 0.0144, 0.0584, 0.4611],\n",
      "         [0.1171, 0.2520, 0.0214,  ..., 0.0263, 0.0638, 0.0406]],\n",
      "\n",
      "        [[0.0750, 0.1642, 0.0074,  ..., 0.0813, 0.0799, 0.1168],\n",
      "         [0.4625, 0.0722, 0.0749,  ..., 0.0175, 0.0666, 0.1324],\n",
      "         [0.2521, 0.1479, 0.0171,  ..., 0.0900, 0.1775, 0.1367],\n",
      "         ...,\n",
      "         [0.0097, 0.0408, 0.3351,  ..., 0.0779, 0.1264, 0.0289],\n",
      "         [0.1251, 0.1357, 0.0362,  ..., 0.1659, 0.2265, 0.1282],\n",
      "         [0.0438, 0.0478, 0.1344,  ..., 0.1139, 0.1279, 0.0787]]])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T19:55:12.757247Z",
     "start_time": "2026-01-19T19:55:12.725467Z"
    }
   },
   "cell_type": "code",
   "source": "sum(scores[0][0])",
   "id": "5c66de4717862c13",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Implement of MASK：",
   "id": "c46130c734146d0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T19:55:24.695286Z",
     "start_time": "2026-01-19T19:55:24.660161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n",
    "mask.size()"
   ],
   "id": "8c84dfb3e17274a7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T19:55:36.397206Z",
     "start_time": "2026-01-19T19:55:36.359694Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size is torch.Size([16, 10, 768])\n",
      "scores size is torch.Size([16, 10, 10])\n",
      "score size is tensor([[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.6394, 0.3606, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0887, 0.0697, 0.8416,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.1735, 0.0828, 0.0813,  ..., 0.1140, 0.0000, 0.0000],\n",
      "         [0.6346, 0.0087, 0.1000,  ..., 0.0367, 0.0416, 0.0000],\n",
      "         [0.0390, 0.0444, 0.5200,  ..., 0.0556, 0.1627, 0.0166]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.7974, 0.2026, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.7007, 0.2202, 0.0791,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0045, 0.0218, 0.0806,  ..., 0.1883, 0.0000, 0.0000],\n",
      "         [0.0131, 0.0636, 0.1953,  ..., 0.0293, 0.1619, 0.0000],\n",
      "         [0.0903, 0.0451, 0.2762,  ..., 0.0544, 0.0705, 0.0963]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.1668, 0.8332, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0662, 0.5312, 0.4026,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0730, 0.1234, 0.1228,  ..., 0.1238, 0.0000, 0.0000],\n",
      "         [0.0421, 0.1471, 0.0281,  ..., 0.0972, 0.1278, 0.0000],\n",
      "         [0.1140, 0.0803, 0.1416,  ..., 0.0423, 0.1479, 0.1398]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.7975, 0.2025, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0532, 0.9242, 0.0226,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.1818, 0.1614, 0.2049,  ..., 0.0629, 0.0000, 0.0000],\n",
      "         [0.0415, 0.0539, 0.0801,  ..., 0.1427, 0.2533, 0.0000],\n",
      "         [0.0354, 0.3169, 0.0580,  ..., 0.0722, 0.0229, 0.0704]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.5158, 0.4842, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3981, 0.1083, 0.4936,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0695, 0.0033, 0.1674,  ..., 0.1573, 0.0000, 0.0000],\n",
      "         [0.4235, 0.0895, 0.0480,  ..., 0.0268, 0.1084, 0.0000],\n",
      "         [0.1171, 0.2520, 0.0214,  ..., 0.0263, 0.0638, 0.0406]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.8650, 0.1350, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.6043, 0.3546, 0.0410,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0115, 0.0483, 0.3967,  ..., 0.0922, 0.0000, 0.0000],\n",
      "         [0.1435, 0.1557, 0.0416,  ..., 0.1903, 0.2598, 0.0000],\n",
      "         [0.0438, 0.0478, 0.1344,  ..., 0.1139, 0.1279, 0.0787]]])\n"
     ]
    }
   ],
   "execution_count": 6,
   "source": [
    "output, scores = sdpa(query, key, value, mask)\n",
    "\n",
    "print(f\"output size is {output.size()}\")\n",
    "print(f\"scores size is {scores.size()}\")\n",
    "print(f\"score size is {scores}\")"
   ],
   "id": "940457a39dbbc97d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Multiheadattention：split the tenser and merge and compute attention",
   "id": "ec5333be52d395c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T19:55:56.147778Z",
     "start_time": "2026-01-19T19:55:56.127225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert self.d_model%self.num_heads==0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.head_dim = self.d_model//self.num_heads\n",
    "\n",
    "        self.query_proj = nn.Linear(d_model, d_model)\n",
    "        self.key_proj = nn.Linear(d_model, d_model)\n",
    "        self.value_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        '''\n",
    "\n",
    "        :param x: [batch_size, seq_len, d_model]\n",
    "        :return:\n",
    "            [batch_size, num_heads, seq_len, head_dim]\n",
    "        '''\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        assert  d_model==self.head_dim*self.num_heads, f\"input must in dim {self.num_heads*self.head_dim} but input dim is {d_model}\"\n",
    "\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "    def combine_heads(self,x):\n",
    "        '''\n",
    "\n",
    "        :param x:  [batch_size, num_heads, seq_len, head_dim]\n",
    "        :return:  [batch_size, seq_len, num_heads*head_dim]\n",
    "        '''\n",
    "        batch_size, num_heads, seq_len, head_dim = x.size()\n",
    "\n",
    "        return x.transpose(1,2).contiguous().view(batch_size, seq_len,num_heads*head_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        query = self.query_proj(x)\n",
    "        key = self.key_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "\n",
    "        splited_query = self.split_heads(query)\n",
    "        splited_key = self.split_heads(key)\n",
    "        splited_value = self.split_heads(value)\n",
    "\n",
    "        output, scores =self.attention(splited_query, splited_key, splited_value, mask)\n",
    "        output = self.combine_heads(output)\n",
    "\n",
    "        return self.out_proj(output), scores"
   ],
   "id": "bf00ab358276c543",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Apply mask on multi head attention",
   "id": "1bc2e6b9f59df573"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T19:56:14.764869Z",
     "start_time": "2026-01-19T19:56:14.738682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MaskedMultiheadAttention(MultiHeadAttention):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MaskedMultiheadAttention, self).__init__(d_model, num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size()[1]\n",
    "        mask = torch.tril(torch.ones(1, seq_len,seq_len))\n",
    "        return super().forward(x, mask)"
   ],
   "id": "96cf21aa0b767fd0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T19:56:27.810930Z",
     "start_time": "2026-01-19T19:56:27.755259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size, seq_len, d_model = 16, 10, 768\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "mmha = MaskedMultiheadAttention(d_model, num_heads=12)\n",
    "output, socres = mmha(x)\n",
    "\n",
    "print(f\"output size is {output.size()}\")\n",
    "print(f\"scores size is {scores.size()}\")\n",
    "print(f\"score size is {scores}\")"
   ],
   "id": "bf3a97eb9b155a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size is torch.Size([16, 10, 768])\n",
      "scores size is torch.Size([16, 10, 10])\n",
      "score size is tensor([[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.6394, 0.3606, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0887, 0.0697, 0.8416,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.1735, 0.0828, 0.0813,  ..., 0.1140, 0.0000, 0.0000],\n",
      "         [0.6346, 0.0087, 0.1000,  ..., 0.0367, 0.0416, 0.0000],\n",
      "         [0.0390, 0.0444, 0.5200,  ..., 0.0556, 0.1627, 0.0166]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.7974, 0.2026, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.7007, 0.2202, 0.0791,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0045, 0.0218, 0.0806,  ..., 0.1883, 0.0000, 0.0000],\n",
      "         [0.0131, 0.0636, 0.1953,  ..., 0.0293, 0.1619, 0.0000],\n",
      "         [0.0903, 0.0451, 0.2762,  ..., 0.0544, 0.0705, 0.0963]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.1668, 0.8332, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0662, 0.5312, 0.4026,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0730, 0.1234, 0.1228,  ..., 0.1238, 0.0000, 0.0000],\n",
      "         [0.0421, 0.1471, 0.0281,  ..., 0.0972, 0.1278, 0.0000],\n",
      "         [0.1140, 0.0803, 0.1416,  ..., 0.0423, 0.1479, 0.1398]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.7975, 0.2025, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0532, 0.9242, 0.0226,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.1818, 0.1614, 0.2049,  ..., 0.0629, 0.0000, 0.0000],\n",
      "         [0.0415, 0.0539, 0.0801,  ..., 0.1427, 0.2533, 0.0000],\n",
      "         [0.0354, 0.3169, 0.0580,  ..., 0.0722, 0.0229, 0.0704]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.5158, 0.4842, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3981, 0.1083, 0.4936,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0695, 0.0033, 0.1674,  ..., 0.1573, 0.0000, 0.0000],\n",
      "         [0.4235, 0.0895, 0.0480,  ..., 0.0268, 0.1084, 0.0000],\n",
      "         [0.1171, 0.2520, 0.0214,  ..., 0.0263, 0.0638, 0.0406]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.8650, 0.1350, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.6043, 0.3546, 0.0410,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0115, 0.0483, 0.3967,  ..., 0.0922, 0.0000, 0.0000],\n",
      "         [0.1435, 0.1557, 0.0416,  ..., 0.1903, 0.2598, 0.0000],\n",
      "         [0.0438, 0.0478, 0.1344,  ..., 0.1139, 0.1279, 0.0787]]])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Feedforward：",
   "id": "d511c8d3468abbcd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardNeuralNetwork, self).__init__()\n",
    "        # laynorm\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        # proj\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        #激活函数\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "\n",
    "        :param x: [batch_szie, seq_len, hidden_size]\n",
    "        :return:\n",
    "        '''\n",
    "        resiual = x\n",
    "        output = self.layer_norm(x) #[batch_szie, seq_len, hidden_size]\n",
    "        output = self.linear1(output) #[batch_szie, seq_len, hidden_size*4]\n",
    "        output = self.activation(output)#[batch_szie, seq_len, hidden_size*4]\n",
    "        output = self.linear2(output)#[batch_szie, seq_len, hidden_size]\n",
    "\n",
    "        return resiual + output"
   ],
   "id": "f4cbef0bf0f2de77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T19:58:51.711769Z",
     "start_time": "2026-01-19T19:58:51.657345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size, seq_len, hidden_size = 16, 10, 768\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "ffn = FeedForwardNeuralNetwork(768, 768*4)\n",
    "\n",
    "output = ffn(x)\n",
    "\n",
    "print(f\"x size is {x.size()}\")\n",
    "print(f\"output size is {output.size()}\")\n",
    "print(f\"output is {output[0]}\")"
   ],
   "id": "c40eb2fba3fd8173",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x size is torch.Size([16, 10, 768])\n",
      "output size is torch.Size([16, 10, 768])\n",
      "output is tensor([[-0.1202,  1.5371, -0.2498,  ...,  0.7882,  0.6324,  0.8899],\n",
      "        [-0.3706, -0.5207,  0.5701,  ...,  1.2478, -1.3609,  0.8454],\n",
      "        [-1.6795, -0.8868,  0.8596,  ...,  0.4456,  0.9892, -0.0972],\n",
      "        ...,\n",
      "        [ 0.0592, -1.9784, -0.6724,  ...,  1.0572,  1.5154,  1.2274],\n",
      "        [ 0.3856,  0.3645,  0.9655,  ..., -0.1415, -1.2560,  0.9342],\n",
      "        [ 0.0223,  0.8291, -0.2557,  ..., -1.3377,  0.1309, -0.4588]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Decoderblock: Feedforward+Multiheadattention",
   "id": "9fe6cc497e9d0563"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T19:59:05.584373Z",
     "start_time": "2026-01-19T19:59:05.564459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super(TransformerDecoderBlock, self).__init__()\n",
    "        #核心模块\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feedForward = FeedForwardNeuralNetwork(d_model, d_ff)\n",
    "        # 后层归一化\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, attn_mask = None):\n",
    "        attn_output, attn_weights = self.attention(x, attn_mask)\n",
    "\n",
    "        ff_output = self.feedForward(x+attn_output)\n",
    "\n",
    "        output = self.layer_norm(ff_output)\n",
    "\n",
    "        return output, attn_weights"
   ],
   "id": "f1829cf7e98b0603",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T19:59:16.851276Z",
     "start_time": "2026-01-19T19:59:16.814315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size, seq_len, hidden_size = 16, 10, 768\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "tdb = TransformerDecoderBlock(hidden_size, 12, hidden_size*4)\n",
    "\n",
    "output, attn_weights = tdb(x)\n",
    "print(f\"x size is {x.size()}\")\n",
    "print(f\"output size is {output.size()}\")\n",
    "print(f\"output is {output[0]}\")"
   ],
   "id": "7e3632a40dcfd6f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x size is torch.Size([16, 10, 768])\n",
      "output size is torch.Size([16, 10, 768])\n",
      "output is tensor([[ 1.0840, -0.0034,  1.2365,  ...,  0.7578,  1.6233, -0.8166],\n",
      "        [-0.2595, -0.8290,  1.1105,  ..., -0.9751,  1.2091,  0.8741],\n",
      "        [-1.7811,  0.0750, -0.6566,  ...,  1.0701,  0.0884, -1.8810],\n",
      "        ...,\n",
      "        [-0.2670, -0.5450, -0.4385,  ...,  1.3385,  0.6984,  0.4419],\n",
      "        [-0.8112, -2.5580,  1.4085,  ..., -0.2154,  1.0498,  0.1947],\n",
      "        [-1.3156,  0.1104, -0.0278,  ..., -0.6975,  0.6842, -0.2235]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Multilayer Decoder:",
   "id": "4ed18150640e4c1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T19:59:29.865671Z",
     "start_time": "2026-01-19T19:59:29.841901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"位置编码模块（支持动态序列长度）\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # [1, max_len, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 动态获取位置编码\n",
    "        position_emb = self.pe[:, :x.size(1)]\n",
    "        return x + position_emb  # [batch, seq_len, d_model]"
   ],
   "id": "e0d44a79e53739a8",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Transfomer Decoder:\n",
    "1. token embedding\n",
    "2. position embedding\n",
    "3. Multihead attention\n",
    "4. Feed forward Nural network\n",
    "5. attention +FFN ：decoder block\n",
    "6. multi-layer-decoder\n",
    "7. laynorm/activation/residual connection"
   ],
   "id": "36206c8cb8438780"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T19:59:50.415889Z",
     "start_time": "2026-01-19T19:59:50.390383Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 17,
   "source": [
    "# Transformer实现\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, num_layers, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "        #堆叠Decoder块\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderBlock(d_model, num_heads, d_ff)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        # tied embeddings\n",
    "        self.output_layer.weight = self.token_embedding.weight\n",
    "\n",
    "        self.init_weights()\n",
    "        pass\n",
    "    def init_weights(self):\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "\n",
    "        #各层做一下初始化\n",
    "        for layer in self.layers:\n",
    "            nn.init.xavier_normal(layer.attention.query_proj.weight)\n",
    "            nn.init.xavier_normal(layer.attention.key_proj.weight)\n",
    "            nn.init.xavier_normal(layer.attention.value_proj.weight)\n",
    "\n",
    "            nn.init.kaiming_normal(layer.feedForward.linear1.weight)\n",
    "            nn.init.kaiming_uniform(layer.feedForward.linear2.weight)\n",
    "\n",
    "    def create_causal_mask(self, seq_len):\n",
    "        mask = torch.tril(torch.ones(seq_len,seq_len))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        '''\n",
    "\n",
    "        :param x:[batch_size, seq_len]\n",
    "        :return:\n",
    "        '''\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "\n",
    "        #嵌入\n",
    "        embeddings = self.token_embedding(input_ids) #[batch_size, seq_len, d_model]\n",
    "        pos_embedding = self.pos_encoder(embeddings)\n",
    "\n",
    "        embeddings = embeddings + pos_embedding\n",
    "\n",
    "        mask = self.create_causal_mask(seq_len)\n",
    "        # 通过所有的Transformer Decoder block\n",
    "        hidden_states = embeddings\n",
    "        all_attn_weights = []\n",
    "        for layer in self.layers:\n",
    "            hidden_states, attn_weights = layer(hidden_states, mask)\n",
    "            all_attn_weights.append(attn_weights)\n",
    "\n",
    "        hidden_states = self.final_norm(hidden_states)\n",
    "\n",
    "        logits = self.output_layer(hidden_states)\n",
    "\n",
    "        return logits, all_attn_weights"
   ],
   "id": "13f14b88a897eacc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T20:00:04.457567Z",
     "start_time": "2026-01-19T20:00:04.352406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = TransformerDecoder(\n",
    "    vocab_size=500, d_model=256, max_len=128, num_layers=12, num_heads=8, d_ff=256*4)"
   ],
   "id": "63ecc571d2cfbae9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_y/mdy8l8_x7dxb2_53nm97ts400000gn/T/ipykernel_98797/1176401213.py:27: FutureWarning: `nn.init.xavier_normal` is now deprecated in favor of `nn.init.xavier_normal_`.\n",
      "  nn.init.xavier_normal(layer.attention.query_proj.weight)\n",
      "/var/folders/_y/mdy8l8_x7dxb2_53nm97ts400000gn/T/ipykernel_98797/1176401213.py:28: FutureWarning: `nn.init.xavier_normal` is now deprecated in favor of `nn.init.xavier_normal_`.\n",
      "  nn.init.xavier_normal(layer.attention.key_proj.weight)\n",
      "/var/folders/_y/mdy8l8_x7dxb2_53nm97ts400000gn/T/ipykernel_98797/1176401213.py:29: FutureWarning: `nn.init.xavier_normal` is now deprecated in favor of `nn.init.xavier_normal_`.\n",
      "  nn.init.xavier_normal(layer.attention.value_proj.weight)\n",
      "/var/folders/_y/mdy8l8_x7dxb2_53nm97ts400000gn/T/ipykernel_98797/1176401213.py:31: FutureWarning: `nn.init.kaiming_normal` is now deprecated in favor of `nn.init.kaiming_normal_`.\n",
      "  nn.init.kaiming_normal(layer.feedForward.linear1.weight)\n",
      "/var/folders/_y/mdy8l8_x7dxb2_53nm97ts400000gn/T/ipykernel_98797/1176401213.py:32: FutureWarning: `nn.init.kaiming_uniform` is now deprecated in favor of `nn.init.kaiming_uniform_`.\n",
      "  nn.init.kaiming_uniform(layer.feedForward.linear2.weight)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T20:00:18.000977Z",
     "start_time": "2026-01-19T20:00:17.955475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size, seq_len = 16,10\n",
    "\n",
    "input_ids = torch.randint(0,500,(batch_size, seq_len))\n",
    "\n",
    "print(f\"input size is {input_ids.size()}\")\n",
    "print(f\"input is {input_ids}\")\n",
    "\n",
    "output, _ = model(input_ids)\n",
    "\n",
    "print(f\"output size is {output.size()}\")\n",
    "print(f\"output is {output[-1]}\")"
   ],
   "id": "2f71a49ec8dc225f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size is torch.Size([16, 10])\n",
      "input is tensor([[ 58, 347, 344,  97, 455, 301, 159, 179, 333, 294],\n",
      "        [470, 389, 478, 340, 150, 375, 448, 131, 492, 111],\n",
      "        [217,  46, 177,  63, 434, 256, 431, 273, 296, 401],\n",
      "        [274, 156, 474, 452,  64, 287, 136, 395,  16,  49],\n",
      "        [171, 484, 118,   6, 145, 463, 405, 178, 424,  35],\n",
      "        [355, 302,  75, 483, 487,  80, 394, 378, 437, 261],\n",
      "        [213, 349, 448, 296, 363, 157, 260, 237, 169, 272],\n",
      "        [232,  25, 351, 485, 236, 251, 412, 428, 174, 421],\n",
      "        [454, 284, 465, 228, 358,  14, 293, 190, 422, 487],\n",
      "        [157, 343, 167, 336, 387,  40,   1, 110, 193, 442],\n",
      "        [ 91, 421, 249, 314,  51, 224, 475, 371, 424,  54],\n",
      "        [152, 242, 360, 339, 400, 210, 489, 441, 242, 285],\n",
      "        [131, 472,  96, 462, 141, 258, 450,  78, 174, 484],\n",
      "        [264, 134,  81,   6,  26, 368, 475,  28, 269, 315],\n",
      "        [411, 321, 207,  42, 237, 361,  31, 397, 155, 363],\n",
      "        [131, 253, 399,  70, 110,  86, 256, 286, 474, 484]])\n",
      "output size is torch.Size([16, 10, 500])\n",
      "output is tensor([[ 0.1645, -0.0124,  0.2247,  ..., -0.5089, -0.2067,  0.0260],\n",
      "        [ 0.1615, -0.0708,  0.2166,  ..., -0.5528, -0.2247,  0.0025],\n",
      "        [ 0.1664, -0.0970,  0.1785,  ..., -0.5789, -0.2384, -0.0015],\n",
      "        ...,\n",
      "        [ 0.2064, -0.0153,  0.1339,  ..., -0.5029, -0.2773,  0.0030],\n",
      "        [ 0.1719, -0.0081,  0.1568,  ..., -0.5241, -0.2653,  0.0160],\n",
      "        [ 0.1460,  0.0172,  0.1466,  ..., -0.5374, -0.2292,  0.0302]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
