{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-04T05:18:07.955259Z",
     "start_time": "2026-01-04T05:18:07.801858Z"
    }
   },
   "source": [
    "class BPETokenizer():\n",
    "    def __init__(self):\n",
    "        self.merge = {}\n",
    "        self.id_to_char = {}\n",
    "        self.char_to_id = {}\n",
    "    def train(self, input_texts, vocab_size):\n",
    "        '''\n",
    "        BPE Algorithm Training\n",
    "        :param input_texts: list of input texts\n",
    "        :param vocab_size: vocabulary size\n",
    "        :return:\n",
    "        '''\n",
    "        #1.tokenize the input\n",
    "        unique_chars = list(set(list(input_texts)))\n",
    "        #2.initialize the base vocabulary\n",
    "        id_to_char = {idx:char for idx, char in enumerate(unique_chars)}\n",
    "        char_to_id = {char:idx for idx, char in enumerate(unique_chars)}\n",
    "        #3.map the input texts to id using vocabulary\n",
    "        ids = [char_to_id[char] for char in input_texts]\n",
    "\n",
    "        merge_times = vocab_size-len(unique_chars)\n",
    "        vocab_size = len(unique_chars)-1\n",
    "        merge = {}\n",
    "        #4.iteratively merge tokens until the vocabulary reaches the target size\n",
    "        for i in range(merge_times):\n",
    "            if len(ids) == 1:\n",
    "                break\n",
    "            # count the frequencies of adjacent subwords\n",
    "            stats = self.stats(ids)\n",
    "\n",
    "            #find the most frequent pair of adjacent subwords\n",
    "            pair = max(stats, key=stats.get)\n",
    "            #update the vocab(bi-direction mapping)\n",
    "            vocab_size += 1\n",
    "            id_to_char[vocab_size] = id_to_char[pair[0]] + id_to_char[pair[1]]\n",
    "            char_to_id[id_to_char[pair[0]] + id_to_char[pair[1]]] = vocab_size\n",
    "            #record the merge Rule\n",
    "            merge[pair] = vocab_size\n",
    "\n",
    "            #merge ids based on the current merges\n",
    "            ids = self.merge_ids(ids, pair, vocab_size)\n",
    "        self.merge = merge\n",
    "        self.id_to_char = id_to_char\n",
    "        self.char_to_id = char_to_id\n",
    "\n",
    "    def stats(self, ids):\n",
    "        '''\n",
    "        count the frequencies of adjacent subwords\n",
    "        :param ids:\n",
    "        :return:\n",
    "        '''\n",
    "        count = {}\n",
    "        for item in zip(ids[:-1], ids[1:]):\n",
    "            count[item] = count.get(item, 0) + 1\n",
    "        return count\n",
    "    def merge_ids(self, ids, pair, idx):\n",
    "        '''\n",
    "        merges adjacent subword pairs in the corpus and updates ids\n",
    "        :param ids: the list of ids before the update\n",
    "        :param pair: the specific pair of ids to be merged\n",
    "        :param idx: the new id assigned to theis merged pair in the vocabulary\n",
    "        :return: a new list of ids with the pairs replaced\n",
    "        '''\n",
    "        new_ids = []\n",
    "        i = 0\n",
    "\n",
    "        while i< len(ids):\n",
    "            if ids[i]==pair[0] and i<len(ids)-1 and ids[i+1]==pair[1]:\n",
    "                new_ids.append(idx)\n",
    "                i+=2\n",
    "            else:\n",
    "                new_ids.append(ids[i])\n",
    "                i+=1\n",
    "        return new_ids\n",
    "\n",
    "    def encode(self, text):\n",
    "        '''\n",
    "        Tokenize a text into a list of ids\n",
    "        :param text:\n",
    "        :return:\n",
    "        '''\n",
    "        #1 segmentation the text\n",
    "        ids = [self.char_to_id[c] for c in text]\n",
    "        print(ids)\n",
    "        #2 perform multiple merges using the merge dictionary to get the final output\n",
    "        while len(ids)>=2:\n",
    "            stats = self.stats(ids)\n",
    "            pair = min(stats, key=lambda p:self.merge.get(p, float('inf')))\n",
    "            if pair not in self.merge:\n",
    "                break\n",
    "            ids = self.merge_ids(ids, pair, self.merge[pair])\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        '''\n",
    "        convert the ids back to text\n",
    "        :param ids:\n",
    "        :return:\n",
    "        '''\n",
    "        return \"\".join([self.id_to_char[index] for index in ids])\n"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T05:28:29.283800Z",
     "start_time": "2026-01-04T05:28:29.257790Z"
    }
   },
   "cell_type": "code",
   "source": "t1 = BPETokenizer()",
   "id": "27d488fbc5ec317f",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T05:28:30.495663Z",
     "start_time": "2026-01-04T05:28:30.455742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_text = \"\"\"\n",
    "    hello, this is a training text. The tokenizer will split the text into words and assign an id\n",
    "    to each word. This is a fantastic world.\n",
    "    \"\"\"\n",
    "t1.train(input_texts=train_text, vocab_size=48)\n",
    "t1.id_to_char"
   ],
   "id": "97bc64780ffea959",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'n',\n",
       " 1: ',',\n",
       " 2: 'l',\n",
       " 3: 'z',\n",
       " 4: 'd',\n",
       " 5: 'g',\n",
       " 6: '\\n',\n",
       " 7: 'f',\n",
       " 8: 'a',\n",
       " 9: 'w',\n",
       " 10: 'e',\n",
       " 11: 'o',\n",
       " 12: 'r',\n",
       " 13: 'p',\n",
       " 14: 'c',\n",
       " 15: 'x',\n",
       " 16: 'h',\n",
       " 17: '.',\n",
       " 18: 'k',\n",
       " 19: ' ',\n",
       " 20: 't',\n",
       " 21: 'T',\n",
       " 22: 'i',\n",
       " 23: 's',\n",
       " 24: '  ',\n",
       " 25: ' t',\n",
       " 26: 's ',\n",
       " 27: 'is ',\n",
       " 28: ' w',\n",
       " 29: '\\n  ',\n",
       " 30: '\\n    ',\n",
       " 31: 'he',\n",
       " 32: 'in',\n",
       " 33: ' wo',\n",
       " 34: ' wor',\n",
       " 35: 'an',\n",
       " 36: 'll',\n",
       " 37: 'his ',\n",
       " 38: 'his is ',\n",
       " 39: 'his is a',\n",
       " 40: ' te',\n",
       " 41: ' tex',\n",
       " 42: ' text',\n",
       " 43: '. ',\n",
       " 44: '. T',\n",
       " 45: 'to',\n",
       " 46: ' word',\n",
       " 47: 'as'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T05:28:59.267722Z",
     "start_time": "2026-01-04T05:28:59.218088Z"
    }
   },
   "cell_type": "code",
   "source": "t1.encode(\"hello world\")",
   "id": "68434ac5a8d37b76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 10, 2, 2, 11, 19, 9, 11, 12, 2, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[31, 36, 11, 34, 2, 4]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T05:29:41.219501Z",
     "start_time": "2026-01-04T05:29:41.191683Z"
    }
   },
   "cell_type": "code",
   "source": "t1.decode([31, 36, 11, 34, 2, 4])",
   "id": "b55ec9f02e5e9e84",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T05:30:07.167428Z",
     "start_time": "2026-01-04T05:30:07.123204Z"
    }
   },
   "cell_type": "code",
   "source": "t1.decode([16, 10, 2, 2, 11, 19, 9, 11, 12, 2, 4])",
   "id": "66e6001c4a0e2264",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
