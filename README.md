From Zero to DeepSeek: Evolution of LLM Theory and Transformer Full Stack Analysis


### Chapter 1: Course Introduction and Environment Setup
- 1-1 Course Introduction and Schedule [Don't Miss]
- 1-2 Why Learn About Large Models and Related Theoretical Knowledge in the AI Era
- 1-3 The Significance of Chinese Developers Learning DeepSeek
- 1-4 Common Learning Resources and Model Downloads for Large Models
- 1-5 Anaconda Introduction and Installation
- 1-6 PyCharm Installation and Remote Server Connection
- 1-7 Following the Map: A Comprehensive Introduction to AI Technology Landscape

### Chapter 2: [Getting Started] DeepSeek Understanding and Experience
- 2-1 The Birth of ChatGPT and DeepSeek's Pursuit
- 2-2 DeepSeek Capability Experience and Impact of Large Models
- 2-3 Getting Started 1 - Building a Powerful Translator Based on DeepSeek Combined with Prompt Engineering
- 2-4 Getting Started 2 - Implementing DeepSeek Distillation Model Private Deployment with Just a Few Lines of Code

### Chapter 3: [Large Model Theory] The Birth Path of DeepSeek
- 3-1 What Problems Does Natural Language Processing Solve
- 3-2 Rule-based and Statistical Methods
- 3-3 Why Use Word Vectors and the Role of Vector Representation
- 3-4 How to Obtain Well-represented Word Vectors?
- 3-5 The Value of Word Vectors
- 3-6 Getting Started: Word Vector Practice
- 3-7 Pre-trained Models (BERT, GPT)
- 3-8 Getting Started: Pre-trained Model Practice
- 3-9 The Birth of Large Language Models
- 3-10 The Birth of DeepSeek
- 3-11 Why Large Models Produce Intelligence

### Chapter 4: [Feature Encoder Transformer] Deep Understanding of Large Model Input and Output
- 4-1 Text Segmentation and Tokens in Large Models
- 4-2 Large Model Tokenizer
- 4-3 Deep Understanding of Tokenizer's Role and Impact
- 4-4 [Getting Started] Tokenizer Practice
- 4-5 Deep Understanding of BPE Algorithm Training and Encoding Process
- 4-6 [Practice] Hand-coding BPE Algorithm Training Code
- 4-7 Initial Understanding of Position Encoding in Large Models
- 4-8 Introduction to Large Model Output Process
- 4-9 Detailed Introduction to Large Model Decoding Principles
- 4-10 [Practice] Finding Optimal Inference Parameters for Large Models Practice

### Chapter 5: [Feature Encoder Transformer] Deep Dive into Attention Mechanism in Transformer
- 5-1 Transformer Basic Knowledge Preparation
- 5-2 [Practice] Hand-coding LayerNorm Code
- 5-3 [Practice] Hand-coding Softmax Code
- 5-4 Deep Understanding of Attention Mechanism
- 5-5 Masked Self-Attention Mechanism
- 5-6 Multi-Head Attention Mechanism
- 5-7 [Practice] Hand-coding Attention Mechanism Code
- 5-8 [Practice] Hand-coding Masked_Self_Attention
- 5-9 [Practice] Hand-coding MaskedMultiHeadAttention Code
- 5-10 Residual Connections and FFN
- 5-11 [Practice] Hand-coding FFN and Residual Structure Implementation Code
- 5-12 [Practice] Hand-coding Transformer Decoder Block Implementation
- 5-13 [Practice] Hand-coding Complete Transformer Code
- 5-14 Evolution of Attention Mechanism: GQA and MQA
- 5-15 [Practice] Hand-coding MQA Attention Mechanism Code
- 5-16 [Practice] Hand-coding GQA Attention Mechanism Code

### Chapter 6: [Feature Encoder Transformer] Deep Dive into Position Encoding in Transformer
- 6-1 Introduction to Relative Position Encoding
- 6-2 Rotary Position Encoding Theory
- 6-3 Hand-coding Rotary Position Encoding ROPE
- 6-4 Core Parameters of Rotary Position Encoding and Their Impact
- 6-5 Variants of Rotary Position Encoding


